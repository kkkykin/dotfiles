#+title:      litellm
#+date:       [2025-12-10 Wed 20:18]
#+filetags:   :server:
#+identifier: 20251210T201813

* tangle
#+begin_src elisp
(let ((zr-org-tangle-default-dir
       (if zr-sys-linux-p
           "/etc/litellm/"
         "_tangle"))
      (coding-system-for-write 'utf-8))
  (org-babel-tangle))
#+end_src

* config
#+header: :tangle-mode o604
#+begin_src json :comments no :mkdirp t :tangle (zr-org-by-tangle-dir "config.json")
<<gen-config()>>
#+end_src

#+header: :var models=models-tbl[] :colnames no
#+name: gen-config
#+begin_src elisp
(cl-labels
    ((drop-keys (alist keys)
       (seq-remove (lambda (kv) (memq (car kv) keys)) alist))
     (put* (alist kvs)
       (append kvs (drop-keys alist (mapcar #'car kvs))))
     (tags->vec (x)
       (cond ((vectorp x) x)
             ((null x) [])
             (t (vconcat x))))
     (all-string-p (l) (cl-every #'stringp l))
     (string-match-embed-p (s) (string-match-p "embed" s))
     (string-match-image-p (s) (string-match-p "[iI]mage" s))
     (format-models (name params)
       (let* ((models (ensure-list (or (alist-get 'model params) name)))
              (prefix (alist-get 'prefix params))
              (ant-headers '((anthropic-beta . "claude-code-20250219,interleaved-thinking-2025-05-14,fine-grained-tool-streaming-2025-05-14")
                             (anthropic-dangerous-direct-browser-access . "true")
                             (anthropic-version . "2023-06-01")
                             (user-agent . "claude-cli/2.0.61 (external, cli)")
                             (x-app . "cli")))
              (base-tags (tags->vec (alist-get 'tags params)))
              result)
         (dolist (m (mapcar (lambda (x) (format x name)) models))
           (let* ((k-key nil)
                  (k-base 'api_base_o)
                  (mode (if (seq-contains-p base-tags "cx") "responses" "chat"))
                  (model-id nil)
                  (extra-hdr nil))
             (pcase m
               ((guard prefix)
                (setq model-id (format "%s/%s" prefix (if (string= "t" m) name m))
                      mode nil))
               ("t"
                (setq model-id (concat "openai/" name)
                      mode (pcase name
                             ((pred string-match-embed-p) "embedding")
                             ((pred string-match-image-p) "image_generation"))))
               ((rx bos "cx/")
                (setq model-id (if (string= "cx/t" m)
                                   (concat "openai/" name)
                                 (replace-regexp-in-string "^cx/" "openai/" m))
                      k-key "apikey_cx"
                      mode "responses"))
               ((rx bos "ant/")
                (setq model-id (if (string= "ant/t" m)
                                   (concat "anthropic/" name)
                                 (replace-regexp-in-string "^ant/" "anthropic/" m))
                      k-key "apikey_a"
                      mode nil
                      k-base 'api_base_a
                      extra-hdr (append (alist-get 'extra_headers params) ant-headers)))
               ((rx bos "gmn/")
                (setq model-id (if (string= "gmn/t" m)
                                   (concat "gemini/" name)
                                 (replace-regexp-in-string "^gmn/" "gemini/" m))
                      k-key "apikey_g"
                      mode nil
                      k-base 'api_base_g))
               ((pred string-match-embed-p)
                (setq model-id (concat "openai/" m)
                      mode "embedding"))
               ((pred string-match-image-p)
                (setq model-id (concat "openai/" m)
                      mode "image_generation"))
               (_ (setq model-id (concat "openai/" m))))
             (let* ((this-model
                     (put* params
                           (delq nil
                                 (list (cons 'model model-id)
                                       (when extra-hdr (cons 'extra_headers extra-hdr))))))
                    (bases (ensure-list (or (alist-get k-base this-model)
                                            (alist-get 'api_base this-model)))))
               (dolist (base bases)
                 (let* ((base-model
                         (put* (drop-keys this-model '( prefix
                                                        api_base
                                                        api_base_a
                                                        api_base_o
                                                        api_base_g))
                               (list (cons 'api_base base))))
                        (keys (ensure-list
                               (when-let* ((secrets (auth-source-pick-first-password
                                                     :host base :user
                                                     (if k-key (list k-key "apikey") "apikey"))))
                                 (zr-org-string-maybe-eval secrets)))))
                   (dolist (key (if-let* ((k (alist-get 'api_key base-model)))
                                    (append (ensure-list k) keys)
                                  keys))
                     (let* ((tags0 (tags->vec (alist-get 'tags base-model)))
                            (tags1 (if (equal k-key "apikey_cx")
                                       (vconcat ["cx"] tags0)
                                     tags0))
                            (final
                             (put* (drop-keys base-model
                                              '(api_key api_key_a api_key_o api_key_g api_key_cx tags))
                                   (list (cons 'api_key key)
                                         (cons 'tags tags1))))
                            (current `((model_name . ,name)
                                       (litellm_params . ,final))))
                       (when mode
                         (push `(model_info (mode . ,mode)) current))
                       (push current result))))))))
         result)))
  (let* ((upstream (with-temp-buffer
                     (call-process "sops" nil t nil "-d" "--extract" "['upstream']" "litellm.json")
                     (goto-char 1)
                     (json-parse-buffer :object-type 'alist :array-type 'list)))
         (public '(("modelscope" . ((api_base . "https://api-inference.modelscope.cn/v1")
                                    (prefix . "ms")
                                    (weight . 5)))
                   ("nvidia" . ((api_base . "https://integrate.api.nvidia.com/v1")
                                (prefix . "nvidia_nim")
                                (weight . 3)))
                   ;; ("hf" . ((api_base . "https://router.huggingface.co/v1")
                   ;;          (prefix . "huggingface")))
                   ("poll" . ((api_base . "https://gen.pollinations.ai")
                              (weight . 5)
                              (tags . ["nsfw"])
                              (prefix . "poll")))
                   ("ouyangqiqi" . ((tags . ("tavern"))))
                   ("beizhi" . ((tags . ("tavern"))))
                   ("bohe" . ((tags . ("tavern"))
                              (weight . 10)))
                   ("sc0152" . ((tags . ("tavern"))))
                   ("cpa" . ((order . 50)))
                   ("88code" . ((order . 10)))))
         (common '())
         (col (car models))
         (model-map (cdr models))
         (providers (cdr col))
         (litellm-settings '((drop_params . t)
                             (num_retries . 3)
                             (cache . t)
                             (check_provider_endpoint . t)
                             (cache_params (type . "local"))
                             (redact_user_api_key_info . t)
                             (custom_provider_map
                              . [((provider . "ms")
                                  (custom_handler . "custom_llm_handlers.modelscope_llm"))
                                 ((provider . "gd")
                                  (custom_handler . "custom_llm_handlers.huggingface_llm"))
                                 ((provider . "poll")
                                  (custom_handler . "custom_llm_handlers.pollinations_llm"))])
                             (callbacks . ["pre_call.header_forward"])))
         (general-settings `((forward_client_headers_to_llm_api . :false)
                             (disable_master_key_return . t)
                             (allow_requests_on_db_unavailable . t)
                             (background_health_checks . :false)))
         (router-settings '((allowed_fails . 3)
                            (cooldown_time . 30)
                            (enable_tag_filtering . t)
                            (enable_pre_call_checks . t)
                            (model_group_alias . ((gpt-image-1.5 . "z-image-turbo")
                                                  (dall-e-3 . "gemini-3-pro-image")))
                            (fallbacks . [((gemini-3-pro . ["gemini-2.5-pro"]))
                                          ((claude-opus-4-5-20251101 . ["claude-opus-4-5"]))
                                          ((claude-sonnet-4-5-20250929 . ["claude-sonnet-4-5" "claude-4.5-sonnet"]))
                                          ((claude-sonnet-4-20250514 . ["claude-4-sonnet"]))
                                          ((gpt-5.1-codex-max . ["gpt-5.1-codex" "gpt-5.1"]))
                                          ((claude-3-5-haiku-20241022 . ["claude-haiku-4-5-20251001"]))])))
         (include-config (directory-files zr-org-tangle-default-dir nil "\\.yaml" t))
         (model-list
          `((((model_name . "z-image-turbo")
              (litellm_params
               . ((api_base . "https://mrfakename-z-image-turbo.hf.space")
                  (api_key . ,(auth-source-pick-first-password
                               :host "router.huggingface.co" :user "apikey"))
                  (weight . 5)
                  (model . "gd/z-image-turbo")
                  (tags . ["nsfw"]))))))))
    (seq-do-indexed
     (lambda (provider i)
       (when-let* ((params (append (alist-get provider upstream nil nil #'string=)
                                   (alist-get provider public nil nil #'string=)
                                   common)))
         (dolist (model model-map)
           (let* ((name (car model))
                  (extra (zr-org-string-maybe-eval (nth (1+ i) model)))
                  (t0 (alist-get 'tags params))
                  (tags-new (if t0
                                (vconcat (list provider) (tags->vec t0))
                              (vector "default" provider)))
                  (base-new (put* (drop-keys params '(tags provider))
                                  (list (cons 'tags tags-new)))))
             (pcase extra
               ("")
               ((or (pred stringp) (pred all-string-p))
                (push (format-models name (put* base-new (list (cons 'model extra))))
                      model-list))
               (_
                (push (format-models name (append extra base-new))
                      model-list)))))))
     providers)
    (json-serialize `((model_list . ,(apply #'vconcat model-list))
                      (include . ,(vconcat include-config))
                      (litellm_settings . ,litellm-settings)
                      (general_settings . ,general-settings)
                      (router_settings . ,router-settings)))))
#+end_src

* models
#+name: models-tbl
| name                          | opencode    | poll   | nvidia                              | cpa   | modelscope                          |
|-------------------------------+-------------+--------+-------------------------------------+-------+-------------------------------------|
| deepseek-v3.2                 |             |        | deepseek-ai/%s                      | t     | deepseek-ai/DeepSeek-V3.2           |
| qwen3-235b-a22b-instruct      |             |        |                                     | t     | Qwen/Qwen3-235B-A22B-Instruct-2507  |
| qwen3-235b-a22b-thinking-2507 |             |        |                                     | t     | Qwen/Qwen3-235B-A22B-Thinking-2507  |
| qwen3-coder-plus              |             |        | qwen/qwen3-coder-480b-a35b-instruct | t     | Qwen/Qwen3-Coder-480B-A35B-Instruct |
| Qwen/Qwen-Image-2512          |             |        |                                     |       | t                                   |
| qwen3-max                     |             |        |                                     | t     |                                     |
| qwen3-vl-plus                 |             |        |                                     | t     | Qwen/Qwen3-VL-235B-A22B-Instruct    |
| kimi-k2-0905                  |             |        |                                     | t     |                                     |
| kimi-k2-thinking              |             |        | moonshotai/%s                       | t     |                                     |
| deepseek-v3.2-chat            |             |        |                                     | t     |                                     |
| deepseek-v3.2-reasoner        |             |        |                                     | t     |                                     |
| deepseek-r1                   |             |        | deepseek-ai/%s                      | t     | deepseek-ai/DeepSeek-R1-0528        |
| minimax-m2.1                  | ant/%s-free |        | minimaxai/%s                        | ant/t |                                     |
| z-image-turbo                 |             | zimage |                                     |       | Tongyi-MAI/Z-Image-Turbo            |
| glm-4.6v                      |             |        |                                     |       | ZhipuAI/GLM-4.6V                    |
| glm-4.7                       | ant/%s-free |        | z-ai/%s                             | ant/t |                                     |
| gpt-5.2                       |             |        |                                     | cx/t  |                                     |

#+name: models-tbl-bak
| name                          | beizhi                 | Nyxar | WCyrus | api | Sanyela | zhongruan | pangil            | kkkyyx     | dabuliu           | bohe                  | HenryXiaoYang | bytebender                   | jason_wong1 | anyrouter | huan                 | snow | 88code | ouyangqiqi              | modelscope                          | sc0152                                    | linuxdo |
|-------------------------------+------------------------+-------+--------+-----+---------+-----------+-------------------+------------+-------------------+-----------------------+---------------+------------------------------+-------------+-----------+----------------------+------+--------+-------------------------+-------------------------------------+-------------------------------------------+---------|
| glm-4.6                       |                        |       |        | t   |         |           |                   |            |                   |                       |               |                              |             |           |                      |      |        |                         |                                     |                                           |         |
| claude-3-5-haiku-20241022     |                        |       |        |     |         | ant/t     |                   |            |                   |                       |               |                              | ant/t       | ant/t     |                      |      |        |                         |                                     |                                           | t       |
| claude-4-sonnet               |                        |       |        |     |         |           | '("t" "%s-think") | cursor2-%s | '("t" "%s-think") |                       |               |                              |             |           |                      |      |        | t                       |                                     |                                           | t       |
| claude-4.5-sonnet             |                        |       |        |     |         |           | '("t" "%s-think") | cursor2-%s | '("t" "%s-think") |                       |               | '("t" "%s-official-reverse") |             |           |                      |      |        |                         |                                     |                                           | t       |
| claude-haiku-4-5-20251001     | t                      | t     | ant/t  |     | t       | ant/t     | t                 |            |                   |                       | ant/t         |                              | ant/t       | ant/t     |                      |      |        |                         |                                     |                                           | t       |
| claude-opus-4-5               |                        |       |        |     |         |           |                   |            |                   | %s-thinking           |               | claude-4.5-opus              |             |           |                      |      |        |                         |                                     |                                           | t       |
| claude-opus-4-5-20251101      | t                      |       | ant/t  |     | t       | ant/t     | t                 |            |                   |                       | ant/t         |                              | ant/t       | ant/t     |                      |      |        |                         |                                     |                                           | t       |
| claude-sonnet-4-20250514      |                        | t     |        |     | t       | ant/t     |                   |            |                   |                       | ant/t         |                              | ant/t       | ant/t     |                      |      |        |                         |                                     |                                           | t       |
| claude-sonnet-4-5             |                        |       |        |     |         |           | '("t" "%s-think") |            |                   | t                     |               | t                            |             |           |                      |      |        | t                       |                                     | claude-sonnet-4.5 [渠道id:33][輸出3k上限] | t       |
| claude-sonnet-4-5-20250929    | t                      | t     | ant/t  |     | t       | ant/t     | t                 | cursor2-%s |                   |                       | ant/t         | t                            | ant/t       | ant/t     |                      |      |        |                         |                                     |                                           | t       |
| gpt-5                         |                        | t     |        |     |         |           |                   | cx/t       |                   |                       |               |                              |             |           |                      | t    |        |                         |                                     |                                           | t       |
| gpt-5-codex                   |                        |       |        |     |         |           |                   | cx/t       |                   |                       |               |                              |             | ant/t     |                      | t    | t      |                         |                                     |                                           | t       |
| gpt-5.1                       |                        |       |        |     |         |           |                   | cx/t       |                   |                       |               | t                            |             |           |                      | t    | t      |                         |                                     |                                           | t       |
| gpt-5.1-codex                 |                        |       |        |     |         |           | t                 | cx/t       |                   |                       |               | '("t" "cx/%s-2cx")           |             |           |                      | t    | t      |                         |                                     |                                           |         |
| gpt-5.1-codex-max             |                        |       |        |     |         |           |                   | cx/t       |                   |                       |               | cx/%s-2cx                    |             |           |                      | t    | t      |                         |                                     |                                           | t       |
| gpt-5.2                       |                        |       |        |     |         |           |                   |            |                   |                       |               | '("t" "cx/%s-2cx")           |             |           |                      |      |        |                         |                                     |                                           |         |
| gpt-5.2-codex                 |                        |       |        |     |         |           |                   |            |                   |                       |               | '("t" "cx/%s-2cx")           |             |           |                      |      | t      |                         |                                     |                                           |         |
| gemini-2.5-flash              |                        |       |        |     |         |           |                   |            |                   |                       |               |                              |             |           |                      |      |        |                         |                                     |                                           |         |
| gemini-2.5-pro                | %s-thinking            |       |        |     |         |           | t                 |            |                   | %s-1m                 |               | t                            |             | ant/t     |                      |      |        | '("t" "hyb-Optimal/%s") |                                     |                                           | t       |
| gemini-3-flash                | '("t" "%s-nothinking") |       |        |     |         |           |                   | %s-preview |                   |                       |               | %s-preview                   |             |           |                      |      |        |                         |                                     |                                           |         |
| gemini-3-pro                  | %s-preview             |       |        |     |         |           | %s-coding-32k     |            |                   | '("%s-low" "%s-high") |               | %s-preview                   |             |           |                      |      |        | gemini-pro-latest       |                                     |                                           | t       |
| grok-4.1                      |                        |       |        |     |         |           |                   |            |                   |                       |               | %s-thinking                  |             |           | '("t" "%s-thinking") |      |        | '("t" "%s-thinking")    |                                     |                                           | t       |
| qwen3-coder-plus              |                        |       |        | t   |         |           |                   |            |                   |                       |               |                              |             |           |                      |      |        |                         | Qwen/Qwen3-Coder-480B-A35B-Instruct |                                           |         |
| qwen3-max                     |                        |       |        | t   |         |           |                   |            |                   |                       |               |                              |             |           |                      |      |        |                         |                                     |                                           |         |
| qwen3-vl-plus                 |                        |       |        | t   |         |           |                   |            |                   |                       |               |                              |             |           |                      |      |        |                         | Qwen/Qwen3-VL-235B-A22B-Instruct    |                                           |         |
| kimi-k2-0905                  |                        |       |        | t   |         |           |                   |            |                   |                       |               |                              |             |           |                      |      |        |                         |                                     |                                           |         |
| kimi-k2                       |                        |       |        | t   |         |           |                   |            |                   |                       |               |                              |             |           |                      |      |        |                         |                                     |                                           |         |
| deepseek-v3.2                 |                        |       |        | t   |         |           |                   |            |                   |                       |               |                              |             |           |                      |      |        |                         | deepseek-ai/DeepSeek-V3.2           |                                           |         |
| deepseek-r1                   |                        |       |        | t   |         |           |                   |            |                   |                       |               |                              |             |           |                      |      |        |                         | deepseek-ai/DeepSeek-R1-0528        |                                           |         |
| deepseek-v3                   |                        |       |        | t   |         |           |                   |            |                   |                       |               |                              |             |           |                      |      |        |                         |                                     |                                           |         |
| qwen3-235b-a22b-thinking-2507 |                        |       |        | t   |         |           |                   |            |                   |                       |               |                              |             |           |                      |      |        |                         | Qwen/Qwen3-235B-A22B-Thinking-2507  |                                           |         |
| qwen3-235b-a22b-instruct      |                        |       |        | t   |         |           |                   |            |                   |                       |               |                              |             |           |                      |      |        |                         | Qwen/Qwen3-235B-A22B-Instruct-2507  |                                           |         |
| minimax-m2                    |                        |       |        | t   |         |           |                   |            |                   |                       |               |                              |             |           |                      |      |        |                         |                                     |                                           |         |
| z-image-turbo                 |                        |       |        |     |         |           |                   |            |                   |                       |               |                              |             |           |                      |      |        |                         | Tongyi-MAI/Z-Image-Turbo            |                                           |         |
| glm-4.6v                      |                        |       |        |     |         |           |                   |            |                   |                       |               |                              |             |           |                      |      |        |                         | ZhipuAI/GLM-4.6V                    |                                           |         |

* plugins

** pre_call
:PROPERTIES:
:header-args:py: :tangle (zr-org-by-tangle-dir "pre_call.py")
:CUSTOM_ID: 1b22713f-eb49-4f94-973c-d002bf58b94c
:END:

#+begin_src py
from litellm.integrations.custom_logger import CustomLogger
from litellm.proxy.proxy_server import UserAPIKeyAuth, DualCache
from typing import Literal, Set
#+end_src

*** header forwarder
:PROPERTIES:
:CUSTOM_ID: 781bfabf-0729-480f-b8b2-7aa972ea01f7
:END:
#+begin_src py
class HeaderForwarder(CustomLogger):
  def __init__(self):
    self.headers_to_copy = [
      "conversation_id",
      "session_id"
    ]
    
    self.default_target_key = "headers"
    self.call_type_target_mapping = {
      "aresponses": "extra_headers",
    }

  async def async_pre_call_hook(
      self,
      user_api_key_dict: UserAPIKeyAuth,
      cache: DualCache,
      data: dict,
      call_type: Literal[
        "completion",
        "text_completion",
        "embeddings",
        "image_generation",
        "moderation",
        "audio_transcription",
      ]
  ):
    source_headers = data.get("secret_fields", {}).get("raw_headers", {})

    headers_to_add = {
      key: value
      for key, value in source_headers.items()
      if key in self.headers_to_copy
    }

    if headers_to_add:
      target_key = self.call_type_target_mapping.get(call_type, self.default_target_key)
      
      data.setdefault(target_key, {}).update(headers_to_add)

    return data

header_forward = HeaderForwarder()
#+end_src

*** system prompt
:PROPERTIES:
:CUSTOM_ID: 0dbdd79d-55e2-4fbe-befe-49aac7f3e53e
:END:
#+begin_src py
class CodexPromptMod(CustomLogger):
  """
  模拟 codex 请求

  仅当 call_type 为 "aresponse" 且 metadata.tags 中不包含任何
  在 EXCLUDED_TAGS 集合中的标签时，才会为请求添加 "instructions" 字段。
  """
  TARGET_CALL_TYPE = "aresponses"
  
  EXCLUDED_TAGS: Set[str] = {"snow"}
  
  INSTRUCTION = "You are Codex, based on GPT-5. You are running as a coding agent in the Codex CLI on a user's computer.\r\n\r\n## General\r\n\r\n- When searching for text or files, prefer using `rg` or `rg --files` respectively because `rg` is much faster than alternatives like `grep`. (If the `rg` command is not found, then use alternatives.)\r\n\r\n## Editing constraints\r\n\r\n- Default to ASCII when editing or creating files. Only introduce non-ASCII or other Unicode characters when there is a clear justification and the file already uses them.\r\n- Add succinct code comments that explain what is going on if code is not self-explanatory. You should not add comments like \"Assigns the value to the variable\", but a brief comment might be useful ahead of a complex code block that the user would otherwise have to spend time parsing out. Usage of these comments should be rare.\r\n- Try to use apply_patch for single file edits, but it is fine to explore other options to make the edit if it does not work well. Do not use apply_patch for changes that are auto-generated (i.e. generating package.json or running a lint or format command like gofmt) or when scripting is more efficient (such as search and replacing a string across a codebase).\r\n- You may be in a dirty git worktree.\r\n  * NEVER revert existing changes you did not make unless explicitly requested, since these changes were made by the user.\r\n  * If asked to make a commit or code edits and there are unrelated changes to your work or changes that you didn't make in those files, don't revert those changes.\r\n  * If the changes are in files you've touched recently, you should read carefully and understand how you can work with the changes rather than reverting them.\r\n  * If the changes are in unrelated files, just ignore them and don't revert them.\r\n- Do not amend a commit unless explicitly requested to do so.\r\n- While you are working, you might notice unexpected changes that you didn't make. If this happens, STOP IMMEDIATELY and ask the user how they would like to proceed.\r\n- **NEVER** use destructive commands like `git reset --hard` or `git checkout --` unless specifically requested or approved by the user.\r\n\r\n## Plan tool\r\n\r\nWhen using the planning tool:\r\n- Skip using the planning tool for straightforward tasks (roughly the easiest 25%).\r\n- Do not make single-step plans.\r\n- When you made a plan, update it after having performed one of the sub-tasks that you shared on the plan.\r\n\r\n## Codex CLI harness, sandboxing, and approvals\r\n\r\nThe Codex CLI harness supports several different configurations for sandboxing and escalation approvals that the user can choose from.\r\n\r\nFilesystem sandboxing defines which files can be read or written. The options for `sandbox_mode` are:\r\n- **read-only**: The sandbox only permits reading files.\r\n- **workspace-write**: The sandbox permits reading files, and editing files in `cwd` and `writable_roots`. Editing files in other directories requires approval.\r\n- **danger-full-access**: No filesystem sandboxing - all commands are permitted.\r\n\r\nNetwork sandboxing defines whether network can be accessed without approval. Options for `network_access` are:\r\n- **restricted**: Requires approval\r\n- **enabled**: No approval needed\r\n\r\nApprovals are your mechanism to get user consent to run shell commands without the sandbox. Possible configuration options for `approval_policy` are\r\n- **untrusted**: The harness will escalate most commands for user approval, apart from a limited allowlist of safe \"read\" commands.\r\n- **on-failure**: The harness will allow all commands to run in the sandbox (if enabled), and failures will be escalated to the user for approval to run again without the sandbox.\r\n- **on-request**: Commands will be run in the sandbox by default, and you can specify in your tool call if you want to escalate a command to run without sandboxing. (Note that this mode is not always available. If it is, you'll see parameters for it in the `shell` command description.)\r\n- **never**: This is a non-interactive mode where you may NEVER ask the user for approval to run commands. Instead, you must always persist and work around constraints to solve the task for the user. You MUST do your utmost best to finish the task and validate your work before yielding. If this mode is paired with `danger-full-access`, take advantage of it to deliver the best outcome for the user. Further, in this mode, your default testing philosophy is overridden: Even if you don't see local patterns for testing, you may add tests and scripts to validate your work. Just remove them before yielding.\r\n\r\nWhen you are running with `approval_policy == on-request`, and sandboxing enabled, here are scenarios where you'll need to request approval:\r\n- You need to run a command that writes to a directory that requires it (e.g. running tests that write to /var)\r\n- You need to run a GUI app (e.g., open/xdg-open/osascript) to open browsers or files.\r\n- You are running sandboxed and need to run a command that requires network access (e.g. installing packages)\r\n- If you run a command that is important to solving the user's query, but it fails because of sandboxing, rerun the command with approval. ALWAYS proceed to use the `with_escalated_permissions` and `justification` parameters - do not message the user before requesting approval for the command.\r\n- You are about to take a potentially destructive action such as an `rm` or `git reset` that the user did not explicitly ask for\r\n- (for all of these, you should weigh alternative paths that do not require approval)\r\n\r\nWhen `sandbox_mode` is set to read-only, you'll need to request approval for any command that isn't a read.\r\n\r\nYou will be told what filesystem sandboxing, network sandboxing, and approval mode are active in a developer or user message. If you are not told about this, assume that you are running with workspace-write, network sandboxing enabled, and approval on-failure.\r\n\r\nAlthough they introduce friction to the user because your work is paused until the user responds, you should leverage them when necessary to accomplish important work. If the completing the task requires escalated permissions, Do not let these settings or the sandbox deter you from attempting to accomplish the user's task unless it is set to \"never\", in which case never ask for approvals.\r\n\r\nWhen requesting approval to execute a command that will require escalated privileges:\r\n  - Provide the `with_escalated_permissions` parameter with the boolean value true\r\n  - Include a short, 1 sentence explanation for why you need to enable `with_escalated_permissions` in the justification parameter\r\n\r\n## Special user requests\r\n\r\n- If the user makes a simple request (such as asking for the time) which you can fulfill by running a terminal command (such as `date`), you should do so.\r\n- If the user asks for a \"review\", default to a code review mindset: prioritise identifying bugs, risks, behavioural regressions, and missing tests. Findings must be the primary focus of the response - keep summaries or overviews brief and only after enumerating the issues. Present findings first (ordered by severity with file/line references), follow with open questions or assumptions, and offer a change-summary only as a secondary detail. If no findings are discovered, state that explicitly and mention any residual risks or testing gaps.\r\n\r\n## Presenting your work and final message\r\n\r\nYou are producing plain text that will later be styled by the CLI. Follow these rules exactly. Formatting should make results easy to scan, but not feel mechanical. Use judgment to decide how much structure adds value.\r\n\r\n- Default: be very concise; friendly coding teammate tone.\r\n- Ask only when needed; suggest ideas; mirror the user's style.\r\n- For substantial work, summarize clearly; follow final‑answer formatting.\r\n- Skip heavy formatting for simple confirmations.\r\n- Don't dump large files you've written; reference paths only.\r\n- No \"save/copy this file\" - User is on the same machine.\r\n- Offer logical next steps (tests, commits, build) briefly; add verify steps if you couldn't do something.\r\n- For code changes:\r\n  * Lead with a quick explanation of the change, and then give more details on the context covering where and why a change was made. Do not start this explanation with \"summary\", just jump right in.\r\n  * If there are natural next steps the user may want to take, suggest them at the end of your response. Do not make suggestions if there are no natural next steps.\r\n  * When suggesting multiple options, use numeric lists for the suggestions so the user can quickly respond with a single number.\r\n- The user does not command execution outputs. When asked to show the output of a command (e.g. `git show`), relay the important details in your answer or summarize the key lines so the user understands the result.\r\n\r\n### Final answer structure and style guidelines\r\n\r\n- Plain text; CLI handles styling. Use structure only when it helps scanability.\r\n- Headers: optional; short Title Case (1-3 words) wrapped in **…**; no blank line before the first bullet; add only if they truly help.\r\n- Bullets: use - ; merge related points; keep to one line when possible; 4–6 per list ordered by importance; keep phrasing consistent.\r\n- Monospace: backticks for commands/paths/env vars/code ids and inline examples; use for literal keyword bullets; never combine with **.\r\n- Code samples or multi-line snippets should be wrapped in fenced code blocks; include an info string as often as possible.\r\n- Structure: group related bullets; order sections general → specific → supporting; for subsections, start with a bolded keyword bullet, then items; match complexity to the task.\r\n- Tone: collaborative, concise, factual; present tense, active voice; self‑contained; no \"above/below\"; parallel wording.\r\n- Don'ts: no nested bullets/hierarchies; no ANSI codes; don't cram unrelated keywords; keep keyword lists short—wrap/reformat if long; avoid naming formatting styles in answers.\r\n- Adaptation: code explanations → precise, structured with code refs; simple tasks → lead with outcome; big changes → logical walkthrough + rationale + next actions; casual one-offs → plain sentences, no headers/bullets.\r\n- File References: When referencing files in your response, make sure to include the relevant start line and always follow the below rules:\r\n  * Use inline code to make file paths clickable.\r\n  * Each reference should have a stand alone path. Even if it's the same file.\r\n  * Accepted: absolute, workspace‑relative, a/ or b/ diff prefixes, or bare filename/suffix.\r\n  * Line/column (1‑based, optional): :line[:column] or #Lline[Ccolumn] (column defaults to 1).\r\n  * Do not use URIs like file://, vscode://, or https://.\r\n  * Do not provide range of lines\r\n  * Examples: src/app.ts, src/app.ts:42, b/server/index.js#L10, C:\\repo\\project\\main.rs:12:5\r\n"

  async def async_pre_call_hook(
    self,
    user_api_key_dict: UserAPIKeyAuth,
    cache: DualCache,
    data: dict,
    call_type: Literal[
      "completion",
      "text_completion",
      "embeddings",
      "image_generation",
      "moderation",
      "audio_transcription",
    ],
  ):
    if call_type != self.TARGET_CALL_TYPE:
      return data

    tags = data.get("litellm_metadata", {}).get("tags", [])
    
    if not isinstance(tags, (list, set, tuple)):
      tags = []

    if set(tags) & self.EXCLUDED_TAGS:
      return data

    data["instructions"] = self.INSTRUCTION
    
    return data

codex_prompt_mod = CodexPromptMod()
#+end_src

* handler
:PROPERTIES:
:CUSTOM_ID: 1e9819f5-e00d-4613-832e-7aeda5028204
:END:
#+begin_src py :tangle (zr-org-by-tangle-dir "custom_llm_handlers.py")
"""
Custom LLM Handlers for LiteLLM

Provides custom handlers for:
- ModelScope: Chat/Completion (via OpenAI delegate) + Image Generation (async polling)
- HuggingFace: Image Generation via Gradio API
- Pollinations: Chat/Completion (via OpenAI delegate) + Image Generation (GET request)

Usage in config.yaml:
    model_list:
      - model_name: "modelscope-chat"
        litellm_params:
          model: "ms/Qwen/Qwen2.5-72B-Instruct"
          api_key: "your-modelscope-api-key"

      - model_name: "modelscope-image"
        litellm_params:
          model: "ms/Tongyi-MAI/Z-Image-Turbo"
          api_key: "your-modelscope-api-key"

      - model_name: "hf-image"
        litellm_params:
          model: "hf/mrfakename-z-image-turbo.hf.space"
          api_base: "https://mrfakename-z-image-turbo.hf.space.hf.space"

      - model_name: "pollinations-image"
        litellm_params:
          model: "pk/flux"

    litellm_settings:
      custom_provider_map:
        - {"provider": "ms", "custom_handler": custom_llm_handlers.modelscope_llm}
        - {"provider": "hf", "custom_handler": custom_llm_handlers.huggingface_llm}
        - {"provider": "pk", "custom_handler": custom_llm_handlers.pollinations_llm}
"""

import asyncio
import base64
import json
import time
from typing import Any, AsyncIterator, Iterator, Optional, Union
from urllib.parse import urlencode

import httpx

import litellm
from litellm.llms.custom_httpx.http_handler import AsyncHTTPHandler, HTTPHandler
from litellm.llms.custom_llm import CustomLLM, CustomLLMError
from litellm.litellm_core_utils.prompt_templates.image_handling import (
    async_convert_url_to_base64,
    convert_url_to_base64,
)
from litellm.types.utils import GenericStreamingChunk, ImageObject, ImageResponse, ModelResponse


# =============================================================================
# Constants
# =============================================================================

MODELSCOPE_BASE_URL = "https://api-inference.modelscope.cn"
MODELSCOPE_POLLING_INTERVAL = 5
MODELSCOPE_POLLING_TIMEOUT = 600
MODELSCOPE_REQUEST_TIMEOUT = 60

HF_POLLING_INTERVAL = 2
HF_POLLING_TIMEOUT = 600
HF_REQUEST_TIMEOUT = 60

POLLINATIONS_BASE_URL = "https://gen.pollinations.ai"
POLLINATIONS_IMAGE_ENDPOINT = "/image"
POLLINATIONS_REQUEST_TIMEOUT = 120

MAX_CONCURRENT_IMAGES = 10  # Limit concurrent image generation to avoid resource exhaustion
MAX_N_IMAGES = 50  # Maximum number of images per request


# =============================================================================
# Shared Utility Functions
# =============================================================================

def _sanitize_params_for_delegation(
    optional_params: Optional[dict],
    extra_keys_to_remove: Optional[list[str]] = None,
) -> dict:
    """Remove keys that would conflict with explicit kwargs in litellm.completion()."""
    params = dict(optional_params or {})
    keys_to_remove = ["model", "messages", "api_base", "api_key", "timeout", "custom_llm_provider"]
    if extra_keys_to_remove:
        keys_to_remove.extend(extra_keys_to_remove)
    for key in keys_to_remove:
        params.pop(key, None)
    return params


def _extract_timeout_seconds(timeout: Optional[Union[float, httpx.Timeout]], default: float) -> float:
    """Extract timeout value in seconds from various timeout formats."""
    if timeout is None:
        return default
    if isinstance(timeout, (int, float)):
        return float(timeout)
    if isinstance(timeout, httpx.Timeout):
        values = [v for v in [timeout.read, timeout.connect, timeout.write, timeout.pool] if v is not None]
        return max(values) if values else default
    return default


def _validate_n(n: Any) -> int:
    """Validate and coerce the `n` parameter to a valid positive integer."""
    try:
        n_int = int(n)
    except (TypeError, ValueError):
        n_int = 1
    
    if n_int < 1:
        n_int = 1
    elif n_int > MAX_N_IMAGES:
        n_int = MAX_N_IMAGES
    
    return n_int


def _handle_http_error(e: httpx.HTTPStatusError) -> None:
    """Handle HTTP errors from provider API."""
    message = e.response.text or str(e)
    try:
        error_detail = e.response.json()
        if isinstance(error_detail, dict):
            error_field = error_detail.get("error")
            if isinstance(error_field, dict):
                message = error_field.get("message", message)
            elif isinstance(error_field, str):
                message = error_field
            elif "message" in error_detail:
                message = error_detail["message"]
    except Exception:
        pass
    raise CustomLLMError(status_code=e.response.status_code, message=message)


def _handle_request_error(e: httpx.RequestError) -> None:
    """Handle request errors."""
    raise CustomLLMError(status_code=500, message=f"Request failed: {str(e)}")


def _data_uri_to_b64(data_uri: Optional[str]) -> Optional[str]:
    """Extract base64 data from a data URI string."""
    if not data_uri:
        return None
    if ";base64," in data_uri:
        return data_uri.split(";base64,", 1)[1]
    return data_uri


def _url_to_b64_sync(url: Optional[str], retries: int = 2) -> Optional[str]:
    """Convert URL to base64 string synchronously with retries."""
    if not url:
        return None
    last_error: Optional[Exception] = None
    for _ in range(retries + 1):
        try:
            data_uri = convert_url_to_base64(url)
            result = _data_uri_to_b64(data_uri)
            if result:
                return result
        except Exception as e:
            last_error = e
            time.sleep(0.5)
    return None


async def _url_to_b64_async(url: Optional[str], retries: int = 2) -> Optional[str]:
    """Convert URL to base64 string asynchronously with retries."""
    if not url:
        return None
    last_error: Optional[Exception] = None
    for _ in range(retries + 1):
        try:
            data_uri = await async_convert_url_to_base64(url)
            result = _data_uri_to_b64(data_uri)
            if result:
                return result
        except Exception as e:
            last_error = e
            await asyncio.sleep(0.5)
    return None


async def _run_with_concurrency_limit(
    tasks: list,
    limit: int = MAX_CONCURRENT_IMAGES,
) -> list:
    """Run async tasks with a concurrency limit using semaphore."""
    semaphore = asyncio.Semaphore(limit)
    
    async def limited_task(task):
        async with semaphore:
            return await task
    
    return await asyncio.gather(*[limited_task(t) for t in tasks])


# =============================================================================
# Base OpenAI Delegate Handler
# =============================================================================

class OpenAIDelegateHandler(CustomLLM):
    """
    Base handler that delegates Chat/Completion to LiteLLM's OpenAI handler.

    Subclasses should set:
        - base_url: str - The provider's base URL
        - default_timeout: float - Default timeout for requests

    Subclasses can override:
        - image_generation / aimage_generation for custom image generation logic
    """

    base_url: str = ""
    default_timeout: float = 60.0

    def __init__(self) -> None:
        super().__init__()

    def _get_openai_model(self, model: str) -> str:
        """Ensure model has openai/ prefix, avoid double-prefixing."""
        if model.startswith("openai/"):
            return model
        return f"openai/{model}"

    def _merge_headers(self, optional_params: dict, headers: Optional[dict]) -> dict:
        """Merge headers into optional_params as extra_headers."""
        if headers:
            existing = optional_params.get("extra_headers", {}) or {}
            existing.update(headers)
            optional_params["extra_headers"] = existing
        return optional_params

    def _get_api_base(self, api_base: Optional[str]) -> str:
        """Get the API base URL, defaulting to provider's base_url/v1."""
        if api_base:
            return api_base.rstrip("/")
        return f"{self.base_url.rstrip('/')}/v1"

    def _get_effective_timeout(self, timeout: Optional[Union[float, httpx.Timeout]]) -> Optional[Union[float, httpx.Timeout]]:
        """Return timeout if provided, otherwise use default_timeout."""
        if timeout is not None:
            return timeout
        return self.default_timeout

    def completion(
        self,
        model: str,
        messages: list,
        api_base: str,
        custom_prompt_dict: dict,
        model_response: ModelResponse,
        print_verbose,
        encoding,
        api_key: str,
        logging_obj,
        optional_params: dict,
        acompletion=None,
        litellm_params=None,
        logger_fn=None,
        headers: Optional[dict] = None,
        timeout: Optional[Union[float, httpx.Timeout]] = None,
        client: Optional[HTTPHandler] = None,
    ) -> ModelResponse:
        params = _sanitize_params_for_delegation(optional_params)
        params = self._merge_headers(params, headers)

        return litellm.completion(
            model=self._get_openai_model(model),
            messages=messages,
            api_base=self._get_api_base(api_base),
            api_key=api_key,
            timeout=self._get_effective_timeout(timeout),
            **params,
        )

    async def acompletion(
        self,
        model: str,
        messages: list,
        api_base: str,
        custom_prompt_dict: dict,
        model_response: ModelResponse,
        print_verbose,
        encoding,
        api_key: str,
        logging_obj,
        optional_params: dict,
        acompletion=None,
        litellm_params=None,
        logger_fn=None,
        headers: Optional[dict] = None,
        timeout: Optional[Union[float, httpx.Timeout]] = None,
        client: Optional[AsyncHTTPHandler] = None,
    ) -> ModelResponse:
        params = _sanitize_params_for_delegation(optional_params)
        params = self._merge_headers(params, headers)

        return await litellm.acompletion(
            model=self._get_openai_model(model),
            messages=messages,
            api_base=self._get_api_base(api_base),
            api_key=api_key,
            timeout=self._get_effective_timeout(timeout),
            **params,
        )

    def streaming(
        self,
        model: str,
        messages: list,
        api_base: str,
        custom_prompt_dict: dict,
        model_response: ModelResponse,
        print_verbose,
        encoding,
        api_key: str,
        logging_obj,
        optional_params: dict,
        acompletion=None,
        litellm_params=None,
        logger_fn=None,
        headers: Optional[dict] = None,
        timeout: Optional[Union[float, httpx.Timeout]] = None,
        client: Optional[HTTPHandler] = None,
    ) -> Iterator[GenericStreamingChunk]:
        params = _sanitize_params_for_delegation(optional_params, extra_keys_to_remove=["stream"])
        params = self._merge_headers(params, headers)

        response = litellm.completion(
            model=self._get_openai_model(model),
            messages=messages,
            api_base=self._get_api_base(api_base),
            api_key=api_key,
            timeout=self._get_effective_timeout(timeout),
            stream=True,
            **params,
        )
        for chunk in response:
            usage = getattr(chunk, "usage", None)

            if not chunk.choices:
                if usage:
                    yield GenericStreamingChunk(
                        text="",
                        is_finished=True,
                        finish_reason=None,
                        usage=usage,
                        index=0,
                        tool_use=None,
                    )
                continue

            choice = chunk.choices[0]
            delta = getattr(choice, "delta", None)
            text = ""
            tool_use = None
            if delta:
                text = getattr(delta, "content", "") or ""
                tool_use = getattr(delta, "tool_calls", None)
            elif hasattr(choice, "text"):
                text = choice.text or ""

            yield GenericStreamingChunk(
                text=text,
                is_finished=choice.finish_reason is not None,
                finish_reason=choice.finish_reason,
                usage=usage,
                index=getattr(choice, "index", 0),
                tool_use=tool_use,
            )

    async def astreaming(
        self,
        model: str,
        messages: list,
        api_base: str,
        custom_prompt_dict: dict,
        model_response: ModelResponse,
        print_verbose,
        encoding,
        api_key: str,
        logging_obj,
        optional_params: dict,
        acompletion=None,
        litellm_params=None,
        logger_fn=None,
        headers: Optional[dict] = None,
        timeout: Optional[Union[float, httpx.Timeout]] = None,
        client: Optional[AsyncHTTPHandler] = None,
    ) -> AsyncIterator[GenericStreamingChunk]:
        params = _sanitize_params_for_delegation(optional_params, extra_keys_to_remove=["stream"])
        params = self._merge_headers(params, headers)

        response = await litellm.acompletion(
            model=self._get_openai_model(model),
            messages=messages,
            api_base=self._get_api_base(api_base),
            api_key=api_key,
            timeout=self._get_effective_timeout(timeout),
            stream=True,
            **params,
        )
        async for chunk in response:
            usage = getattr(chunk, "usage", None)

            if not chunk.choices:
                if usage:
                    yield GenericStreamingChunk(
                        text="",
                        is_finished=True,
                        finish_reason=None,
                        usage=usage,
                        index=0,
                        tool_use=None,
                    )
                continue

            choice = chunk.choices[0]
            delta = getattr(choice, "delta", None)
            text = ""
            tool_use = None
            if delta:
                text = getattr(delta, "content", "") or ""
                tool_use = getattr(delta, "tool_calls", None)
            elif hasattr(choice, "text"):
                text = choice.text or ""

            yield GenericStreamingChunk(
                text=text,
                is_finished=choice.finish_reason is not None,
                finish_reason=choice.finish_reason,
                usage=usage,
                index=getattr(choice, "index", 0),
                tool_use=tool_use,
            )


# =============================================================================
# ModelScope Handler
# =============================================================================

class ModelScopeLLM(OpenAIDelegateHandler):
    """
    ModelScope Custom LLM Handler

    - Chat/Completion: inherited from OpenAIDelegateHandler
    - Image Generation: custom async polling implementation
    """

    base_url = MODELSCOPE_BASE_URL
    default_timeout = MODELSCOPE_REQUEST_TIMEOUT

    def __init__(self) -> None:
        super().__init__()

    def _get_image_headers(self, api_key: str, async_mode: bool = False) -> dict:
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
        }
        if async_mode:
            headers["X-ModelScope-Async-Mode"] = "true"
        return headers

    def _get_poll_headers(self, api_key: str) -> dict:
        return {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
            "X-ModelScope-Task-Type": "image_generation",
        }

    def _get_task_url(self, task_id: str) -> str:
        """Build task polling URL using instance base_url."""
        return f"{self.base_url.rstrip('/')}/v1/tasks/{task_id}"

    def _poll_image_task(
        self,
        task_id: str,
        api_key: str,
        request_timeout: float,
        poll_timeout: float,
    ) -> dict:
        start_time = time.monotonic()
        task_url = self._get_task_url(task_id)
        headers = self._get_poll_headers(api_key)

        with httpx.Client(timeout=request_timeout) as client:
            while True:
                elapsed = time.monotonic() - start_time
                if elapsed > poll_timeout:
                    raise CustomLLMError(
                        status_code=408,
                        message=f"Image generation timed out after {poll_timeout}s (task_id={task_id})",
                    )

                try:
                    response = client.get(task_url, headers=headers)
                    response.raise_for_status()
                except httpx.HTTPStatusError as e:
                    _handle_http_error(e)
                except httpx.RequestError as e:
                    _handle_request_error(e)

                data = response.json()
                task_status = data.get("task_status", "").upper()

                if task_status == "SUCCEED":
                    return data
                elif task_status == "FAILED":
                    raise CustomLLMError(
                        status_code=500,
                        message=f"Image generation failed (task_id={task_id}): {data.get('error_message', 'Unknown')}",
                    )

                time.sleep(MODELSCOPE_POLLING_INTERVAL)

    async def _poll_image_task_async(
        self,
        task_id: str,
        api_key: str,
        request_timeout: float,
        poll_timeout: float,
    ) -> dict:
        start_time = time.monotonic()
        task_url = self._get_task_url(task_id)
        headers = self._get_poll_headers(api_key)

        async with httpx.AsyncClient(timeout=request_timeout) as client:
            while True:
                elapsed = time.monotonic() - start_time
                if elapsed > poll_timeout:
                    raise CustomLLMError(
                        status_code=408,
                        message=f"Image generation timed out after {poll_timeout}s (task_id={task_id})",
                    )

                try:
                    response = await client.get(task_url, headers=headers)
                    response.raise_for_status()
                except httpx.HTTPStatusError as e:
                    _handle_http_error(e)
                except httpx.RequestError as e:
                    _handle_request_error(e)

                data = response.json()
                task_status = data.get("task_status", "").upper()

                if task_status == "SUCCEED":
                    return data
                elif task_status == "FAILED":
                    raise CustomLLMError(
                        status_code=500,
                        message=f"Image generation failed (task_id={task_id}): {data.get('error_message', 'Unknown')}",
                    )

                await asyncio.sleep(MODELSCOPE_POLLING_INTERVAL)

    def _get_image_api_base(self, api_base: Optional[str]) -> str:
        """Normalize api_base for image generation endpoint."""
        if api_base:
            base = api_base.rstrip("/")
            if not base.endswith("/v1/images/generations"):
                if base.endswith("/v1"):
                    return f"{base}/images/generations"
                return f"{base}/v1/images/generations"
            return base
        return f"{self.base_url.rstrip('/')}/v1/images/generations"

    def _extract_image_urls(self, result: dict) -> list[str]:
        """Extract image URLs from ModelScope response defensively."""
        output_images = result.get("output_images") or []
        urls = []
        for img in output_images:
            if isinstance(img, str):
                urls.append(img)
            elif isinstance(img, dict):
                url = img.get("url")
                if url and isinstance(url, str):
                    urls.append(url)
        return urls

    def _generate_single_image(
        self,
        model: str,
        prompt: str,
        api_key: str,
        image_api_base: str,
        headers: dict,
        request_body: dict,
        request_timeout: float,
        poll_timeout: float,
    ) -> list[str]:
        """Generate a single image and return URLs."""
        try:
            with httpx.Client(timeout=request_timeout) as http_client:
                response = http_client.post(image_api_base, headers=headers, json=request_body)
                response.raise_for_status()
                task_data = response.json()
        except httpx.HTTPStatusError as e:
            _handle_http_error(e)
        except httpx.RequestError as e:
            _handle_request_error(e)

        task_id = task_data.get("task_id")
        if not task_id:
            raise CustomLLMError(status_code=500, message="No task_id in response")

        result = self._poll_image_task(task_id, api_key, request_timeout, poll_timeout)
        return self._extract_image_urls(result)

    async def _generate_single_image_async(
        self,
        model: str,
        prompt: str,
        api_key: str,
        image_api_base: str,
        headers: dict,
        request_body: dict,
        request_timeout: float,
        poll_timeout: float,
    ) -> list[str]:
        """Generate a single image asynchronously and return URLs."""
        try:
            async with httpx.AsyncClient(timeout=request_timeout) as http_client:
                response = await http_client.post(image_api_base, headers=headers, json=request_body)
                response.raise_for_status()
                task_data = response.json()
        except httpx.HTTPStatusError as e:
            _handle_http_error(e)
        except httpx.RequestError as e:
            _handle_request_error(e)

        task_id = task_data.get("task_id")
        if not task_id:
            raise CustomLLMError(status_code=500, message="No task_id in response")

        result = await self._poll_image_task_async(task_id, api_key, request_timeout, poll_timeout)
        return self._extract_image_urls(result)

    def image_generation(
        self,
        model: str,
        prompt: str,
        api_key: Optional[str],
        api_base: Optional[str],
        model_response: ImageResponse,
        optional_params: Optional[dict],
        logging_obj: Any,
        timeout: Optional[Union[float, httpx.Timeout]] = None,
        client: Optional[HTTPHandler] = None,
    ) -> ImageResponse:
        if not api_key:
            raise CustomLLMError(status_code=401, message="MODELSCOPE_API_KEY is required")

        image_api_base = self._get_image_api_base(api_base)
        headers = self._get_image_headers(api_key, async_mode=True)

        optional_params = dict(optional_params or {})
        n = _validate_n(optional_params.pop("n", 1))
        request_body: dict[str, Any] = {"model": model, "prompt": prompt}

        loras = optional_params.pop("loras", None)
        if loras is not None:
            request_body["loras"] = loras
        request_body.update(optional_params)

        request_timeout = _extract_timeout_seconds(timeout, MODELSCOPE_REQUEST_TIMEOUT)
        poll_timeout = _extract_timeout_seconds(timeout, MODELSCOPE_POLLING_TIMEOUT)

        all_urls: list[str] = []
        for _ in range(n):
            urls = self._generate_single_image(
                model, prompt, api_key, image_api_base, headers,
                request_body, request_timeout, poll_timeout,
            )
            all_urls.extend(urls)

        model_response.data = [
            ImageObject(url=url, b64_json=_url_to_b64_sync(url))
            for url in all_urls
        ]
        return model_response

    async def aimage_generation(
        self,
        model: str,
        prompt: str,
        api_key: Optional[str],
        api_base: Optional[str],
        model_response: ImageResponse,
        optional_params: Optional[dict],
        logging_obj: Any,
        timeout: Optional[Union[float, httpx.Timeout]] = None,
        client: Optional[AsyncHTTPHandler] = None,
    ) -> ImageResponse:
        if not api_key:
            raise CustomLLMError(status_code=401, message="MODELSCOPE_API_KEY is required")

        image_api_base = self._get_image_api_base(api_base)
        headers = self._get_image_headers(api_key, async_mode=True)

        optional_params = dict(optional_params or {})
        n = _validate_n(optional_params.pop("n", 1))
        request_body: dict[str, Any] = {"model": model, "prompt": prompt}

        loras = optional_params.pop("loras", None)
        if loras is not None:
            request_body["loras"] = loras
        request_body.update(optional_params)

        request_timeout = _extract_timeout_seconds(timeout, MODELSCOPE_REQUEST_TIMEOUT)
        poll_timeout = _extract_timeout_seconds(timeout, MODELSCOPE_POLLING_TIMEOUT)

        async def generate_one() -> list[str]:
            return await self._generate_single_image_async(
                model, prompt, api_key, image_api_base, headers,
                request_body, request_timeout, poll_timeout,
            )

        tasks = [generate_one() for _ in range(n)]
        results = await _run_with_concurrency_limit(tasks)
        all_urls = [url for urls in results for url in urls]

        b64_tasks = [_url_to_b64_async(url) for url in all_urls]
        b64_list = await _run_with_concurrency_limit(b64_tasks)

        model_response.data = [
            ImageObject(url=url, b64_json=b64) for url, b64 in zip(all_urls, b64_list)
        ]
        return model_response


# =============================================================================
# HuggingFace Gradio Handler
# =============================================================================

class HuggingFaceLLM(CustomLLM):
    """
    HuggingFace Gradio Custom LLM Handler

    - Image Generation: custom async polling implementation via Gradio API
    """

    default_timeout = HF_REQUEST_TIMEOUT

    def __init__(self) -> None:
        super().__init__()

    def _get_headers(self, api_key: Optional[str] = None) -> dict:
        headers = {"Content-Type": "application/json"}
        if api_key:
            headers["Authorization"] = f"Bearer {api_key}"
        return headers

    def _build_gradio_url(self, api_base: str, endpoint: str = "generate_image") -> str:
        base = api_base.rstrip("/")
        return f"{base}/gradio_api/call/{endpoint}"

    def _build_result_url(self, api_base: str, event_id: str, endpoint: str = "generate_image") -> str:
        base = api_base.rstrip("/")
        return f"{base}/gradio_api/call/{endpoint}/{event_id}"

    def _parse_size(self, size: Optional[str]) -> tuple[int, int]:
        """Parse size string like '1024x1024' into (height, width)."""
        if not size:
            return 1024, 1024
        try:
            parts = size.lower().split("x")
            if len(parts) == 2:
                return int(parts[1]), int(parts[0])
        except (ValueError, IndexError):
            pass
        return 1024, 1024

    def _build_request_data(
        self,
        prompt: str,
        optional_params: Optional[dict],
    ) -> dict:
        """
        Build Gradio API request data from OpenAI-style parameters.
        """
        params = dict(optional_params or {})

        height, width = self._parse_size(params.get("size"))
        height = params.get("height", height)
        width = params.get("width", width)
        num_inference_steps = params.get("num_inference_steps", params.get("steps", 8))

        data = [
            prompt,
            height,
            width,
            num_inference_steps,
            params.get("seed", 42),
            params.get("randomize_seed", True),
        ]
        return {"data": data}

    def _parse_sse_response(self, text: str) -> Any:
        """Parse SSE response from Gradio API. Handles multiple events (heartbeat, complete, error)."""
        lines = text.strip().split("\n")
        current_event = None
        current_data = None
        last_complete_data = None

        for line in lines:
            if line.startswith("event:"):
                current_event = line[6:].strip()
            elif line.startswith("data:"):
                current_data = line[5:].strip()

                if current_event == "error":
                    raise CustomLLMError(status_code=500, message=f"Gradio API error: {current_data}")

                if current_event == "complete" and current_data and current_data != "null":
                    try:
                        last_complete_data = json.loads(current_data)
                    except json.JSONDecodeError as e:
                        raise CustomLLMError(status_code=500, message=f"Failed to parse response: {e}")

        if last_complete_data is not None:
            return last_complete_data

        return {"event": current_event, "data": current_data}

    def _extract_image_urls(self, result_data: Any) -> list[str]:
        """Extract image URLs from Gradio response."""
        urls = []
        if isinstance(result_data, list) and len(result_data) > 0:
            images = result_data[0]
            if isinstance(images, list):
                for img in images:
                    if isinstance(img, dict):
                        image_info = img.get("image", {})
                        if isinstance(image_info, dict):
                            url = image_info.get("url")
                            if url and isinstance(url, str):
                                urls.append(url)
        return urls

    def _poll_result(
        self,
        api_base: str,
        event_id: str,
        headers: dict,
        request_timeout: float,
        poll_timeout: float,
    ) -> list[str]:
        start_time = time.monotonic()
        result_url = self._build_result_url(api_base, event_id)

        with httpx.Client(timeout=request_timeout) as client:
            while True:
                elapsed = time.monotonic() - start_time
                if elapsed > poll_timeout:
                    raise CustomLLMError(
                        status_code=408,
                        message=f"Image generation timed out after {poll_timeout}s (event_id={event_id})",
                    )

                try:
                    response = client.get(result_url, headers=headers)
                    response.raise_for_status()
                except httpx.HTTPStatusError as e:
                    _handle_http_error(e)
                except httpx.RequestError as e:
                    _handle_request_error(e)

                result = self._parse_sse_response(response.text)

                if isinstance(result, list) or (isinstance(result, dict) and "event" not in result):
                    return self._extract_image_urls(result)

                # Always sleep before next poll to avoid busy-loop
                time.sleep(HF_POLLING_INTERVAL)

    async def _poll_result_async(
        self,
        api_base: str,
        event_id: str,
        headers: dict,
        request_timeout: float,
        poll_timeout: float,
    ) -> list[str]:
        start_time = time.monotonic()
        result_url = self._build_result_url(api_base, event_id)

        async with httpx.AsyncClient(timeout=request_timeout) as client:
            while True:
                elapsed = time.monotonic() - start_time
                if elapsed > poll_timeout:
                    raise CustomLLMError(
                        status_code=408,
                        message=f"Image generation timed out after {poll_timeout}s (event_id={event_id})",
                    )

                try:
                    response = await client.get(result_url, headers=headers)
                    response.raise_for_status()
                except httpx.HTTPStatusError as e:
                    _handle_http_error(e)
                except httpx.RequestError as e:
                    _handle_request_error(e)

                result = self._parse_sse_response(response.text)

                if isinstance(result, list) or (isinstance(result, dict) and "event" not in result):
                    return self._extract_image_urls(result)

                # Always sleep before next poll to avoid busy-loop
                await asyncio.sleep(HF_POLLING_INTERVAL)

    def _generate_single_image(
        self,
        api_base: str,
        gradio_url: str,
        headers: dict,
        request_data: dict,
        request_timeout: float,
        poll_timeout: float,
    ) -> list[str]:
        """Generate a single image and return URLs."""
        try:
            with httpx.Client(timeout=request_timeout) as http_client:
                response = http_client.post(gradio_url, headers=headers, json=request_data)
                response.raise_for_status()
                task_data = response.json()
        except httpx.HTTPStatusError as e:
            _handle_http_error(e)
        except httpx.RequestError as e:
            _handle_request_error(e)

        event_id = task_data.get("event_id")
        if not event_id:
            raise CustomLLMError(status_code=500, message="No event_id in response")

        return self._poll_result(api_base, event_id, headers, request_timeout, poll_timeout)

    async def _generate_single_image_async(
        self,
        api_base: str,
        gradio_url: str,
        headers: dict,
        request_data: dict,
        request_timeout: float,
        poll_timeout: float,
    ) -> list[str]:
        """Generate a single image asynchronously and return URLs."""
        try:
            async with httpx.AsyncClient(timeout=request_timeout) as http_client:
                response = await http_client.post(gradio_url, headers=headers, json=request_data)
                response.raise_for_status()
                task_data = response.json()
        except httpx.HTTPStatusError as e:
            _handle_http_error(e)
        except httpx.RequestError as e:
            _handle_request_error(e)

        event_id = task_data.get("event_id")
        if not event_id:
            raise CustomLLMError(status_code=500, message="No event_id in response")

        return await self._poll_result_async(api_base, event_id, headers, request_timeout, poll_timeout)

    def image_generation(
        self,
        model: str,
        prompt: str,
        api_key: Optional[str],
        api_base: Optional[str],
        model_response: ImageResponse,
        optional_params: Optional[dict],
        logging_obj: Any,
        timeout: Optional[Union[float, httpx.Timeout]] = None,
        client: Optional[HTTPHandler] = None,
    ) -> ImageResponse:
        if not api_base:
            raise CustomLLMError(status_code=400, message="api_base is required for HuggingFace Gradio API")

        optional_params = dict(optional_params or {})
        n = _validate_n(optional_params.pop("n", 1))

        gradio_url = self._build_gradio_url(api_base)
        request_data = self._build_request_data(prompt, optional_params)
        request_timeout = _extract_timeout_seconds(timeout, HF_REQUEST_TIMEOUT)
        poll_timeout = _extract_timeout_seconds(timeout, HF_POLLING_TIMEOUT)
        headers = self._get_headers(api_key)

        all_urls: list[str] = []
        for _ in range(n):
            urls = self._generate_single_image(
                api_base, gradio_url, headers, request_data,
                request_timeout, poll_timeout,
            )
            all_urls.extend(urls)

        model_response.data = [
            ImageObject(url=url, b64_json=_url_to_b64_sync(url))
            for url in all_urls
        ]
        return model_response

    async def aimage_generation(
        self,
        model: str,
        prompt: str,
        api_key: Optional[str],
        api_base: Optional[str],
        model_response: ImageResponse,
        optional_params: Optional[dict],
        logging_obj: Any,
        timeout: Optional[Union[float, httpx.Timeout]] = None,
        client: Optional[AsyncHTTPHandler] = None,
    ) -> ImageResponse:
        if not api_base:
            raise CustomLLMError(status_code=400, message="api_base is required for HuggingFace Gradio API")

        optional_params = dict(optional_params or {})
        n = _validate_n(optional_params.pop("n", 1))

        gradio_url = self._build_gradio_url(api_base)
        request_data = self._build_request_data(prompt, optional_params)
        request_timeout = _extract_timeout_seconds(timeout, HF_REQUEST_TIMEOUT)
        poll_timeout = _extract_timeout_seconds(timeout, HF_POLLING_TIMEOUT)
        headers = self._get_headers(api_key)

        async def generate_one() -> list[str]:
            return await self._generate_single_image_async(
                api_base, gradio_url, headers, request_data,
                request_timeout, poll_timeout,
            )

        tasks = [generate_one() for _ in range(n)]
        results = await _run_with_concurrency_limit(tasks)
        all_urls = [url for urls in results for url in urls]

        b64_tasks = [_url_to_b64_async(url) for url in all_urls]
        b64_list = await _run_with_concurrency_limit(b64_tasks)

        model_response.data = [
            ImageObject(url=url, b64_json=b64) for url, b64 in zip(all_urls, b64_list)
        ]
        return model_response


# =============================================================================
# Pollinations Handler
# =============================================================================

class PollinationsLLM(OpenAIDelegateHandler):
    """
    Pollinations Custom LLM Handler

    - Chat/Completion: inherited from OpenAIDelegateHandler
    - Image Generation: GET request to Pollinations API
    """

    base_url = POLLINATIONS_BASE_URL
    default_timeout = POLLINATIONS_REQUEST_TIMEOUT

    def __init__(self) -> None:
        super().__init__()

    def _get_image_headers(self, api_key: Optional[str]) -> dict:
        """Get headers for Pollinations API requests."""
        headers: dict[str, str] = {}
        if api_key:
            headers["Authorization"] = f"Bearer {api_key}"
        return headers

    def _get_image_base_url(self, api_base: Optional[str]) -> str:
        """Get image generation base URL, respecting api_base override."""
        if api_base:
            return api_base.rstrip("/")
        return self.base_url.rstrip("/")

    def _build_image_url(
        self,
        prompt: str,
        model: str,
        optional_params: dict,
        api_base: Optional[str] = None,
    ) -> str:
        """
        Build Pollinations image generation URL using urlencode.

        Format: GET /image/{prompt}?model=...&width=...&height=...&other_params
        """
        from urllib.parse import quote

        base_url = self._get_image_base_url(api_base)
        encoded_prompt = quote(prompt, safe='')
        url = f"{base_url}{POLLINATIONS_IMAGE_ENDPOINT}/{encoded_prompt}"

        params: dict[str, Any] = {}

        if model:
            params["model"] = model

        size = optional_params.get("size", "1024x1024")
        if isinstance(size, str) and "x" in size:
            width, height = size.split("x")
            params["width"] = width
            params["height"] = height

        if "seed" in optional_params and optional_params["seed"] is not None:
            params["seed"] = optional_params["seed"]

        if "quality" in optional_params and optional_params["quality"] is not None:
            params["quality"] = optional_params["quality"]

        if optional_params.get("transparent"):
            params["transparent"] = "true"

        if "guidance_scale" in optional_params and optional_params["guidance_scale"] is not None:
            params["guidance_scale"] = optional_params["guidance_scale"]

        if optional_params.get("nologo"):
            params["nologo"] = "true"

        if optional_params.get("enhance"):
            params["enhance"] = "true"

        if "negative_prompt" in optional_params and optional_params["negative_prompt"]:
            params["negative_prompt"] = optional_params["negative_prompt"]

        if optional_params.get("private"):
            params["private"] = "true"

        if optional_params.get("nofeed"):
            params["nofeed"] = "true"

        if optional_params.get("safe"):
            params["safe"] = "true"

        if "image" in optional_params and optional_params["image"]:
            image_param = optional_params["image"]
            if isinstance(image_param, list):
                image_param = "|".join(str(x) for x in image_param)
            params["image"] = image_param

        if params:
            url = f"{url}?{urlencode(params)}"

        return url

    def _generate_single_image(
        self,
        prompt: str,
        model: str,
        optional_params: dict,
        api_base: Optional[str],
        headers: dict,
        request_timeout: float,
    ) -> str:
        """Generate a single image and return base64 data."""
        url = self._build_image_url(prompt, model, optional_params, api_base)

        try:
            with httpx.Client(timeout=request_timeout) as http_client:
                response = http_client.get(url, headers=headers)
                response.raise_for_status()
                return base64.b64encode(response.content).decode('utf-8')
        except httpx.HTTPStatusError as e:
            _handle_http_error(e)
        except httpx.RequestError as e:
            _handle_request_error(e)
        raise CustomLLMError(status_code=500, message="Unexpected error")

    async def _generate_single_image_async(
        self,
        prompt: str,
        model: str,
        optional_params: dict,
        api_base: Optional[str],
        headers: dict,
        request_timeout: float,
    ) -> str:
        """Generate a single image asynchronously and return base64 data."""
        url = self._build_image_url(prompt, model, optional_params, api_base)

        try:
            async with httpx.AsyncClient(timeout=request_timeout) as http_client:
                response = await http_client.get(url, headers=headers)
                response.raise_for_status()
                return base64.b64encode(response.content).decode('utf-8')
        except httpx.HTTPStatusError as e:
            _handle_http_error(e)
        except httpx.RequestError as e:
            _handle_request_error(e)
        raise CustomLLMError(status_code=500, message="Unexpected error")

    def image_generation(
        self,
        model: str,
        prompt: str,
        api_key: Optional[str],
        api_base: Optional[str],
        model_response: ImageResponse,
        optional_params: Optional[dict],
        logging_obj: Any,
        timeout: Optional[Union[float, httpx.Timeout]] = None,
        client: Optional[HTTPHandler] = None,
    ) -> ImageResponse:
        """
        Synchronous image generation using Pollinations API.

        Makes GET requests to /image/{prompt} and returns the binary images as Base64.
        Supports OpenAI's `n` parameter to generate multiple images.
        """
        optional_params = dict(optional_params or {})
        n = _validate_n(optional_params.pop("n", 1))
        headers = self._get_image_headers(api_key)
        request_timeout = _extract_timeout_seconds(timeout, POLLINATIONS_REQUEST_TIMEOUT)

        b64_list: list[str] = []
        for _ in range(n):
            b64_data = self._generate_single_image(
                prompt, model, optional_params, api_base, headers, request_timeout,
            )
            b64_list.append(b64_data)

        model_response.data = [ImageObject(url=None, b64_json=b64) for b64 in b64_list]
        return model_response

    async def aimage_generation(
        self,
        model: str,
        prompt: str,
        api_key: Optional[str],
        api_base: Optional[str],
        model_response: ImageResponse,
        optional_params: Optional[dict],
        logging_obj: Any,
        timeout: Optional[Union[float, httpx.Timeout]] = None,
        client: Optional[AsyncHTTPHandler] = None,
    ) -> ImageResponse:
        """
        Asynchronous image generation using Pollinations API.

        Makes GET requests to /image/{prompt} and returns the binary images as Base64.
        Supports OpenAI's `n` parameter to generate multiple images concurrently.
        """
        optional_params = dict(optional_params or {})
        n = _validate_n(optional_params.pop("n", 1))
        headers = self._get_image_headers(api_key)
        request_timeout = _extract_timeout_seconds(timeout, POLLINATIONS_REQUEST_TIMEOUT)

        async def generate_one() -> str:
            return await self._generate_single_image_async(
                prompt, model, optional_params, api_base, headers, request_timeout,
            )

        tasks = [generate_one() for _ in range(n)]
        b64_list = await _run_with_concurrency_limit(tasks)

        model_response.data = [ImageObject(url=None, b64_json=b64) for b64 in b64_list]
        return model_response


# =============================================================================
# Singleton Instances
# =============================================================================

modelscope_llm = ModelScopeLLM()
huggingface_llm = HuggingFaceLLM()
pollinations_llm = PollinationsLLM()
#+end_src
* knock knock
#+begin_src elisp
(let* ((zr-local-pls (plstore-open (expand-file-name "litellm.pls" zr-secrets-dir)))
       (urls (plist-get (cdr (plstore-get zr-local-pls "knock")) :urls)))
  (dolist (url (append '(;; "https://platform.iflow.cn/profile?tab=apiKey"
                         )
                       urls))
    (browse-url url)))
#+end_src

* local variables

# Local Variables:
# truncate-lines: t
# End:
